exec_env := "das6"

export BENCHMARK_JAR := if exec_env == "local" {
  "../target/scala-2.13/provxlib-assembly-0.1.0-SNAPSHOT.jar"
} else if exec_env == "das5" {
  "provxlib-assembly-0.1.0-SNAPSHOT.jar"
} else if exec_env == "das6" {
  "provxlib-assembly-0.1.0-SNAPSHOT.jar"
} else {
  error("unknown environment")
}

export EXPERIMENTS_DIR := if exec_env == "local" {
  "/Users/gm/vu/thesis/impl/provxlib/run/experiments"
} else if exec_env == "das5" {
  "/var/scratch/gmo520/thesis/experiments"
} else if exec_env == "das6" {
  "/var/scratch/gmo520/thesis/experiments"
} else {
  error("unknown environment")
}

export DOWNLOAD_DIR := if exec_env == "local" {
  "/Users/gm/vu/thesis/data/raw"
} else if exec_env == "das5" {
  "/var/scratch/gmo520/thesis/data/raw"
} else if exec_env == "das6" {
  "/var/scratch/gmo520/thesis/data/raw"
} else {
  error("unknown environment")
}

export LOCAL_DATASET_DIR := if exec_env == "local" {
  "/Users/gm/vu/thesis/data/graphs"
} else if exec_env == "das5" {
  "/var/scratch/gmo520/thesis/data/graphs"
} else if exec_env == "das6" {
  "/var/scratch/gmo520/thesis/data/graphs"
} else {
  error("unknown environment")
}

export RUNNER_CONFIG := if exec_env == "local" {
  "/Users/gm/vu/thesis/impl/provxlib/run/config.local.properties"
} else if exec_env == "das5" {
  "/home/gmo520/provxlib/run/config.das5.conf"
} else if exec_env == "das6" {
  "/home/gmo520/provxlib/run/config.das6.conf"
} else {
  error("unknown environment")
}


# Environment variables required for Spark setup
@env:
  touch env
  cat /home/gmo520/spark-exec/provx-$(squeue -u gmo520 | tail -n 1 | awk '{ print $1 }').out | grep -P 'export (JAVA|SPARK|HADOOP)' | sed 's/^\*\s*//g' | sort -u > env
  echo 'export PATH="$HADOOP_HOME/bin:$SPARK_HOME/bin:$PATH"' >> env

# Copy graph datasets to HDFS
copy: env
  #!/usr/bin/env bash
  source ./env
  files=$(just datasets | xargs -I{} find $LOCAL_DATASET_DIR -type f -name '{}*')
  hadoop fs -mkdir -p /graphs
  for f in $files; do
    if hadoop fs -ls /graphs/$(basename $f) > /dev/null; then
      echo "File $(basename $f) exists, skipping..."
    else
      echo "Copying $(basename $f)..."
      hadoop fs -copyFromLocal -f $f /graphs &
    fi
  done
  wait

# DESTRUCTIVE: delete experiments directory
clean-local: env
  #!/usr/bin/env bash
  source ./env
  # TODO: ask for user to confirm before proceeding
  rm -r $EXPERIMENTS_DIR
  mkdir -p $EXPERIMENTS_DIR

# Clean HDFS directories
clean-hdfs: env
  #!/usr/bin/env bash
  source ./env
  dirs=(
    /lineage
    /results
    /output
    /spark-logs
  )
  for dir in ${dirs[@]}; do
    hadoop fs -rm -r -f $dir
    hadoop fs -mkdir -p $dir
  done

clean: clean-hdfs #clean-local

# Clean directories and copy graphs to HDFS
setup: download decompress clean copy

# Run benchmark
bench description: env
  #!/usr/bin/env bash
  source ./env
  exec 3>&1 4>&2
  output=$(just provx "{{ description }}" 2> >(tee /dev/fd/4) | tee /dev/fd/3)
  exec 3>&- 4>&-
  # post
  output_files=$(echo $output | grep 'RUNNER OUTPUT')
  runner_stdout=$(echo $output_files | awk '{ print $3 }')
  runner_stderr=$(echo $output_files | awk '{ print $4 }')
  ARCHIVE_FOLDER=$(echo $output | grep 'Experiment path' | tail -n 1 | cut -d ':' -f 2 | sed 's/ //')
  cp $runner_stdout $runner_stderr $ARCHIVE_FOLDER

# Run benchmark
dry: env
  #!/usr/bin/env bash
  source ./env
  just provx "dry run" --dry-run true

# Resume benchmark
resume date: env
  #!/usr/bin/env bash
  source ./env
  # Dry run benchmark to show which configurations will be run
  read -n 1 -p "Continue? " bla
  spark-submit --class lu.magalhaes.gilles.provxlib.benchmark.Runner $BENCHMARK_JAR --config $RUNNER_CONFIG --description "continue {{ date }}" --resume "{{ date }}" > >(tee runner.stdout.log) 2> >(tee runner.stderr.log >&2)
  # post
  ARCHIVE_FOLDER=$(cat runner.stdout.log | grep 'Experiments path' | tail -n 1 | cut -d ':' -f 2 | sed 's/ //')
  cp runner.stdout.log runner.stderr.log $ARCHIVE_FOLDER

provx description *args:
  #!/usr/bin/env bash
  fid=$(tr -dc A-Za-z0-9 < /dev/urandom | head -c 4)
  stdoutfile=runner-${fid}.stdout.log
  stderrfile=runner-${fid}.stderr.log
  spark-submit --class lu.magalhaes.gilles.provxlib.benchmark.Runner $BENCHMARK_JAR --config $RUNNER_CONFIG --description "{{description}}" {{args}} > >(tee $stdoutfile) 2> >(tee $stderrfile >&2)
  echo RUNNER OUTPUT $stdoutfile $stderrfile

datasets:
  #!/usr/bin/env bash
  grep -zPo 'graphs\s+=\s+\[[^\]]+\]' $RUNNER_CONFIG | sed '1d;$d' | awk '{$1=$1;print}' | grep -v '#' | sed '/^$/d'

# Decompress compressed graph datasets
decompress:
  #!/bin/bash

  mkdir -p $LOCAL_DATASET_DIR

  for file in $DOWNLOAD_DIR/*.tar.zst; do
    if [ ! -f $LOCAL_DATASET_DIR/$(basename $file .tar.zst).v ]; then
      echo "Decompressing $file"
      tar -C $LOCAL_DATASET_DIR --use-compress-program=unzstd -xvf $file &
    fi
  done
  wait

# Download graph datasets from LDBC Graphalytics
download:
  #!/bin/bash

  base_url="https://pub-383410a98aef4cb686f0c7601eddd25f.r2.dev/graphalytics/"

  datasets=$(just datasets | awk '{print $0 ".tar.zst"}')

  for dataset in ${datasets[@]}; do
    dataset_path=$DOWNLOAD_DIR/$dataset
    if [ ! -f $dataset_path ]; then
      url=$base_url$dataset
      echo "Downloading $url"
      wget -P $DOWNLOAD_DIR $url &
    fi
  done

  wait
