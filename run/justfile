export BENCHMARK_JAR := "../target/scala-2.12/provxlib-assembly-0.1.0-SNAPSHOT.jar"
export EXPERIMENTS_DIR := "/Users/gm/vu/thesis/impl/provxlib/run/experiments"
export DOWNLOAD_DIR := "/Users/gm/vu/thesis/data/raw"
export LOCAL_DATASET_DIR := "/Users/gm/vu/thesis/data/graphs"
export RUNNER_CONFIG := "/Users/gm/vu/thesis/impl/provxlib/run/config.properties"

# Environment variables required for Spark setup
@env:
	touch env
	# echo 'export JAVA_HOME="/usr/lib/jvm/jre-1.8.0/"' > env
	# cat /home/gmo520/spark-exec/provx-$(squeue -u gmo520 | tail -n 1 | awk '{ print $1 }').out | grep -P 'export (SPARK_|HADOOP_)' | sed 's/^\*\s*//g' >> env
	# echo 'export PATH="$HADOOP_HOME/bin:$PATH"' >> env
	# echo 'export PATH="$SPARK_HOME/bin:$PATH"' >> env

# Copy graph datasets to HDFS
copy: env
	#!/usr/bin/env bash
	source ./env
	hadoop fs -copyFromLocal $LOCAL_DATASET_DIR /graphs

clean-local: env
  #!/usr/bin/env bash
  source ./env
  rm -r $EXPERIMENTS_DIR
  mkdir -p $EXPERIMENTS_DIR


# Clean HDFS directories
clean-hdfs: env
	#!/usr/bin/env bash
	source ./env
	dirs=(
	  /user/$USER/checkpoints
	  /user/$USER/results
	  /user/$USER/output
	  /user/$USER/spark-logs
	)
	for dir in ${dirs[@]}; do
	  hadoop fs -rm -r -f $dir
	  hadoop fs -mkdir -p $dir
	done

clean: clean-hdfs clean-local

setup: clean copy

# Run benchmark
bench description: env
  #!/usr/bin/env bash
  source ./env
  just -d .. -f ../justfile
  cp $BENCHMARK_JAR provxlib.jar
  spark-submit --driver-memory 8G --executor-memory 8G --class lu.magalhaes.gilles.provxlib.benchmark.Runner provxlib.jar --config $RUNNER_CONFIG --description "{{ description }}" > >(tee runner.stdout.log) 2> >(tee runner.stderr.log >&2)

post: sizes
	#!/usr/bin/env bash
	ARCHIVE_FOLDER=$(cat runner.stdout.log | grep 'Experiments path' | tail -n 1 | cut -d ':' -f 2 | sed 's/ //')
	cp runner.stdout.log runner.stderr.log $ARCHIVE_FOLDER

# Compute size of lineage for experiments
sizes: env
	#!/usr/bin/env bash
	source ./env
	ARCHIVE_FOLDER=$(cat runner.stdout.log | grep 'Experiments path' | tail -n 1 | cut -d ':' -f 2 | sed 's/ //')
	ids=$(find $ARCHIVE_FOLDER -name 'metrics.json' | xargs jq -r '.metadata.lineageDirectory' | grep -v 'null')

	checkpoint_sizes_file=$ARCHIVE_FOLDER/checkpoint_sizes.txt
	output_sizes_file=$ARCHIVE_FOLDER/output_sizes.txt

	rm -f $checkpoint_sizes_file $output_sizes_file
	touch $checkpoint_sizes_file $output_sizes_file

	echo $ids
	for id in $ids; do
		hadoop fs -du -s /user/gmo520/checkpoints/$id >> $checkpoint_sizes_file
		hadoop fs -du -s '/user/gmo520/checkpoints/'$id'/*' >> $checkpoint_sizes_file
	done

	runs=$(hadoop fs -ls /user/gmo520/output | awk '{ print $8 }')
	echo $runs
	for run in $runs; do
		hadoop fs -du $run >> $output_sizes_file
	done

# Decompress compressed graph datasets
decompress:
	#!/bin/bash

	mkdir -p $LOCAL_DATASET_DIR
	
	for file in $DOWNLOAD_DIR/*.tar.zst; do
		if [ ! -f $LOCAL_DATASET_DIR/$(basename $file .tar.zst).v ]; then
			echo "Decompressing $file"
			tar -C $LOCAL_DATASET_DIR --use-compress-program=unzstd -xvf $file &
		fi
	done
	
	wait 

# Download graph datasets from LDBC Graphalytics
download:
	#!/bin/bash
	
	base_url="https://pub-383410a98aef4cb686f0c7601eddd25f.r2.dev/graphalytics/"

	datasets=(
		# XS datasets
		"kgs.tar.zst"
		"wiki-Talk.tar.zst"
		"cit-Patents.tar.zst"
	
		# S datasets
		# "datagen-7_5-fb.tar.zst"
		# "datagen-7_6-fb.tar.zst"
		# "datagen-7_7-zf.tar.zst"
		# "datagen-7_8-zf.tar.zst"
		# "datagen-7_9-fb.tar.zst"
		# "dota-league.tar.zst"
		# "graph500-22.tar.zst"
	)
	for dataset in ${datasets[@]}; do
		dataset_path=$DOWNLOAD_DIR/$dataset
		if [ ! -f $dataset_path ]; then
			url=$base_url$dataset
			echo "Downloading $url"
			wget -P $DOWNLOAD_DIR $url &
		fi
	done
	
	wait 
