exec_env := "das6"

export BENCHMARK_JAR := if exec_env == "local" {
  "../target/scala-2.13/provxlib-assembly-0.1.0-SNAPSHOT.jar"
} else if exec_env == "das5" {
  "provxlib-assembly-0.1.0-SNAPSHOT.jar"
} else if exec_env == "das6" {
  "provxlib-assembly-0.1.0-SNAPSHOT.jar"
} else {
  error("unknown environment")
}

export EXPERIMENTS_DIR := if exec_env == "local" {
  "/Users/gm/vu/thesis/impl/provxlib/run/experiments"
} else if exec_env == "das5" {
  "/var/scratch/gmo520/thesis/experiments"
} else if exec_env == "das6" {
  "/var/scratch/gmo520/thesis/experiments"
} else {
  error("unknown environment")
}

export DOWNLOAD_DIR := if exec_env == "local" {
  "/Users/gm/vu/thesis/data/raw"
} else if exec_env == "das5" {
  "/var/scratch/gmo520/thesis/data/raw"
} else if exec_env == "das6" {
  "/var/scratch/gmo520/thesis/data/raw"
} else {
  error("unknown environment")
}

export LOCAL_DATASET_DIR := if exec_env == "local" {
  "/Users/gm/vu/thesis/data/graphs"
} else if exec_env == "das5" {
  "/var/scratch/gmo520/thesis/data/graphs"
} else if exec_env == "das6" {
  "/local/gmo520/graphs"
} else {
  error("unknown environment")
}

#export RUNNER_CONFIG := if exec_env == "local" {
#  "/Users/gm/vu/thesis/impl/provxlib/run/config.local.properties"
#} else if exec_env == "das5" {
#  "/home/gmo520/provxlib/run/config.das5.conf"
#} else if exec_env == "das6" {
#  "/home/gmo520/provxlib/run/wow.conf"
#} else {
#  error("unknown environment")
#}

# "/home/gmo520/provxlib/run/config.das6.conf"

env-check:
  #!/usr/bin/env bash

  if [ -z ${RUNNER_CONFIG+x} ]; then
    echo "RUNNER_CONFIG is unset."
    exit 1
  fi

# Environment variables required for Spark setup
@env:
  if [ -z ${ENV_FILE+x} ]; then
    echo "ENV_FILE is unset, using default `env`."
    ENV_FILE="env"
  fi

  touch $ENV_FILE
  cat /home/gmo520/spark-exec/provx-yarn-$(squeue -u gmo520 | tail -n 1 | awk '{ print $1 }').out | grep -P 'export (JAVA|SPARK|HADOOP)' | sed 's/^\*\s*//g' | sort -u > $ENV_FILE
  echo 'export PATH="$HADOOP_HOME/bin:$SPARK_HOME/bin:$PATH"' >> $ENV_FILE

# Copy graph datasets to HDFS
copy: env
  #!/usr/bin/env bash
  source ./env
  files=$(just datasets | xargs -I{} find $LOCAL_DATASET_DIR -type f -name '{}*')
  hadoop fs -mkdir -p /graphs
  for f in $files; do
    if hadoop fs -ls /graphs/$(basename $f) > /dev/null; then
      echo "File $(basename $f) exists, skipping..."
    else
      echo "Copying $(basename $f)..."
      hadoop fs -copyFromLocal -f $f /graphs &
    fi
  done
  wait

# DESTRUCTIVE: delete experiments directory
clean-local: env
  #!/usr/bin/env bash
  source ./env
  # TODO: ask for user to confirm before proceeding
  rm -r $EXPERIMENTS_DIR
  mkdir -p $EXPERIMENTS_DIR

# Clean HDFS directories
clean-hdfs: env
  #!/usr/bin/env bash
  source ./env
  dirs=(
    /lineage
    /results
    /output
    /spark-logs
  )
  for dir in ${dirs[@]}; do
    hadoop fs -rm -r -f $dir
    hadoop fs -mkdir -p $dir
  done

clean: clean-hdfs #clean-local

# Clean directories and copy graphs to HDFS
setup: download decompress clean copy

# Run benchmark
bench description: env-check env
  #!/usr/bin/env bash
  source ./env
  just provx "{{ description }}"

# Run benchmark
dry: env
  #!/usr/bin/env bash
  source ./env
  just provx "dry run" --dry-run true

# Resume benchmark
resume date: env-check env
  #!/usr/bin/env bash
  source ./env
  # Dry run benchmark to show which configurations will be run
  read -n 1 -p "Continue? " bla
  just provx "continue {{ date }}" --resume "{{ date }}"

provx description *args: env-check
  #!/usr/bin/env bash
  fid=$(tr -dc A-Za-z0-9 < /dev/urandom | head -c 4)
  stdoutfile=runner-${fid}.stdout.log
  stderrfile=runner-${fid}.stderr.log
  spark-submit --master yarn --class lu.magalhaes.gilles.provxlib.benchmark.Runner $BENCHMARK_JAR --config $RUNNER_CONFIG --description "{{description}}" {{args}} > >(tee $stdoutfile) 2> >(tee $stderrfile >&2)
  ARCHIVE_FOLDER=$(cat $stdoutfile | grep 'Experiment path' | tail -n 1 | cut -d ':' -f 2 | sed 's/ //')
  mv $stdoutfile $stderrfile $ARCHIVE_FOLDER

datasets: env-check
  #!/usr/bin/env bash
  grep -zPo 'graphs\s+=\s+\[[^\]]+\]' $RUNNER_CONFIG | sed '1d;$d' | awk '{$1=$1;print}' | grep -v '//' | sed '/^$/d' | sed 's/"//g' | sed 's/,$//g'

# Decompress compressed graph datasets
decompress:
  #!/bin/bash

  mkdir -p $LOCAL_DATASET_DIR

  for file in $DOWNLOAD_DIR/*.tar.zst; do
    if [ ! -f $LOCAL_DATASET_DIR/$(basename $file .tar.zst).v ]; then
      echo "Decompressing $file"
      tar -C $LOCAL_DATASET_DIR --use-compress-program=unzstd -xvf $file &
    fi
  done
  wait

# Download graph datasets from LDBC Graphalytics
download: env-check
  #!/bin/bash

  base_url="https://pub-383410a98aef4cb686f0c7601eddd25f.r2.dev/graphalytics/"

  datasets=$(just datasets | awk '{print $0 ".tar.zst"}')

  for dataset in ${datasets[@]}; do
    dataset_path=$DOWNLOAD_DIR/$dataset
    if [ ! -f $dataset_path ]; then
      url=$base_url$dataset
      echo "Downloading $url"
      wget -P $DOWNLOAD_DIR $url &
    fi
  done

  wait
